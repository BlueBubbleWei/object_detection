###########################################
# data parameters
###########################################
--- !data
image_shape: [384, 384]

datasets:
  - name: ava
    data_dir: /media/storage/datasets/products/TrainData
    train_files: "train_data/*/train.txt"
    tfrecord_files: tfrecords/*.records
    weight: 1.
    overwrite_tfrecord: true

###########################################
# training parameters
###########################################
--- !train
is_training: true
model_dir: './models'
num_epochs: 0
learning_rate: 0.001
learning_rate_decay:
  decay_steps: 3000
  decay_rate: 0.98

batch_size: 32
shuffle: true

augmentation:
  random_rotate: true
  random_brightness: true
  random_contrast: true

optimizer:
  name: adam
  params:
    beta1: 0.9
    beta2: 0.999
    epsilon: 1e-8

filenames_shuffle_buffer_size: 100
num_parallel_map_calls: 12  # ~ num of CPU cores or less
num_readers: 32
read_block_length: 64
shuffle_buffer_size: 4096
prefetch_size: 64  # ~ 2 x batch-size


bbox_clf_weight: 1.
bbox_reg_weight: 1.

quantize: False

###########################################
# model parameters
###########################################
--- !model
model_name: mobilenet_obj
input_shape: [384, 384]
output_shape: [48, 48]
# output_stride: 8

depth_multiplier: 1.
min_depth: 8

# in top-down format
base_anchor_sizes: [32, 64, 128]
anchor_strides: [8, 16, 32]

anchor_scales: [1., 1.2599, 1.5874]
anchor_ratios: [.5, 1., 2.]

unmatched_threshold: 0.4
matched_threshold: 0.6
force_match_for_gt_bbox: true
scale_factors: [10., 5.]

skip_layers:
  - layer_7
  - layer_11
  - layer_17

fpn_depth: 96


###########################################
# inference parameters
###########################################
--- !infer
model_dir: models/latest
frozen_model: frozen_model.pb
network_input_shape: [384, 384]
products_csv: '/media/storage/datasets/products/products.csv'

# out_stride determines the output shape of network
# For example, if input image is 320 x 320
# With stride = 8, output heatmap is 40 x 40
out_stride: 8

# raw image is downsized to resize_shape [H, W]
# NOTE : Tips for resizing:
# * For speed, downsize image as much as possible
#   but this usually results in loss of accuracy
# * For best speed-accuracy compromise, resize image
#   so that network_input_shape dimension is
#   2 to 4 times bbox dimension
#   where dimension is defined as sqrt(H * W)
# * Try to preserve aspect ratio as far as possible
# Example:
#   [320, 569] for lab videos
#   [456, 800] for Walmart videos
resize_shape: [384, 384]

# overlapping patches are encouraged
# for more robust prediction near edge of patches
# overlap is determined by the strides
# which is the distance between two overlapping patches
# [strides-y, strides-x]
# Example:
#   [1, 249] for lab videos
#   [136, 240] for Walmart videos
strides: [1, 106]

# input_type can be images, video or camera
input_type: camera
# images can be a list or use *
# images: /media/easystore/TrainData/Walmart/Round1/Recording_2/20180308_*.jpg
images: /media/easystore/TrainData/Lab/April20/Recording_44/20180420_*.jpg

video: /media/easystore/TrainData/Lab/April20/Recording_44/20180420_0000000.avi

# camera: http://root:123456@192.168.1.24/mjpg/video.mjpg
camera: http://root:amrelirox@192.168.1.24/mjpg/video.mjpg

display_bbox: true

